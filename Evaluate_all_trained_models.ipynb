{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "direct-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import shutil\n",
    "import uuid\n",
    "from tensorflow.keras import backend as K\n",
    "from nn_tools import read_count\n",
    "import tensorflow as tf\n",
    "from data_bank import data_selector\n",
    "import model_builders as mb\n",
    "import os\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from adversarial_attacks.spsa import spsa, spsa_T1\n",
    "from adversarial_attacks.df_attacks import attack_network\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as res_prep\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_prep\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "innocent-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Demo_evaluate_network import print_certainty_vs_distance_table\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-joint",
   "metadata": {},
   "source": [
    "# Iterate over models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "physical-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Set up computational resource \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '2'\n",
    "\n",
    "# Turn on soft memory allocation\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.log_device_placement = False\n",
    "sess = tf.compat.v1.Session(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collectible-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "union-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config_files/create_images_shades.yml')as ymlfile:\n",
    "    cgf1 = yaml.load(ymlfile, Loader =yaml.SafeLoader)\n",
    "\n",
    "\n",
    "data_loader = data_selector('existing', cgf1['DATA']['arguments'])\n",
    "data_test, labels_test, diff_test = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model:  800\n",
      "input_shape (224, 224, 1)\n",
      "Evaluate on training data\n",
      "10000/10000 [==============================] - 52s 5ms/step - loss: 0.3566 - sparse_categorical_accuracy: 0.9827\n",
      "Loss :0.35655948120778014, Accuracy: 0.9827%.\n",
      "Evaluate on testing data\n",
      "5000/5000 [==============================] - 27s 5ms/step - loss: 0.6931 - sparse_categorical_accuracy: 0.5052\n",
      "Loss :0.6931471824645996, Accuracy: 0.5052%.\n",
      "Checking model:  500\n",
      "input_shape (224, 224, 1)\n",
      "Evaluate on training data\n",
      "10000/10000 [==============================] - 1738s 174ms/step - loss: 2.9148 - sparse_categorical_accuracy: 0.7856\n",
      "Loss :2.914763532138698, Accuracy: 0.7856%.\n",
      "Evaluate on testing data\n",
      "5000/5000 [==============================] - 872s 174ms/step - loss: 4.6296 - sparse_categorical_accuracy: 0.5086\n",
      "Loss :4.629649094330826, Accuracy: 0.5086%.\n",
      "Checking model:  303\n",
      "input_shape (224, 224, 1)\n",
      "Evaluate on training data\n",
      "10000/10000 [==============================] - 2110s 211ms/step - loss: 0.9639 - sparse_categorical_accuracy: 0.9364\n",
      "Loss :0.963905312142036, Accuracy: 0.9364%.\n",
      "Evaluate on testing data\n",
      "4906/5000 [============================>.] - ETA: 19s - loss: 7.9769 - sparse_categorical_accuracy: 0.5051"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir('./model'):\n",
    "    \n",
    "    \n",
    "    # Remove hidden files\n",
    "    if filename.startswith('.'):\n",
    "        break\n",
    "    \n",
    "    print('Checking model: ',filename)\n",
    "    \n",
    "    configfile = './model/'+ filename +'/config.yml'\n",
    "\n",
    "    with open(configfile) as ymlfile:\n",
    "        cgf = yaml.load(ymlfile, Loader =yaml.SafeLoader)\n",
    "        \n",
    "    data_loader = data_selector('existing', cgf['DATASET_TRAIN']['arguments'])\n",
    "    \n",
    "    data, labels, diff = data_loader.load_data() \n",
    "    \n",
    "    \n",
    "    # Get input and output shape\n",
    "    input_shape = data.shape[1:]\n",
    "    #output_shape = labels.shape[1];\n",
    "    print('input_shape', input_shape)\n",
    "    # Set the default precision \n",
    "    model_precision = cgf['MODEL_METADATA']['precision']\n",
    "    K.set_floatx(model_precision)\n",
    "\n",
    "    model_id = cgf['MODEL_METADATA']['model_number_arguments']['model_id']\n",
    "    model_path = join('model', str(model_id))\n",
    "    filepath = cgf['MODEL_METADATA']['save_best_model']['arguments']['filepath']\n",
    "\n",
    "    #original_data = cgf['DATASET']['arguments']['original_images']\n",
    "    #bels = cgf['DATASET']['arguments']['labels']\n",
    "\n",
    "    weights_path = './model/' + filename + '/'+ filepath\n",
    "\n",
    "    optimizer = cgf['TRAIN']['optim']['type']\n",
    "    loss_type = cgf['TRAIN']['loss']['type']\n",
    "    metric_list = list(cgf['TRAIN']['metrics'].values())\n",
    "\n",
    "    if loss_type == 'SparseCategoricalCrossentropy':\n",
    "        loss_type = SparseCategoricalCrossentropy(from_logits=False)\n",
    "        metric_list = [SparseCategoricalAccuracy()]\n",
    "        output_shape = 2\n",
    "        labels = np.reshape(labels, (-1))\n",
    "\n",
    "    model_name = cgf['MODEL']['name']\n",
    "    model_arguments = cgf['MODEL']['arguments']\n",
    "    #model = mb.model_selector(model_name, input_shape, output_shape, model_arguments)\n",
    "\n",
    "    model = tf.keras.models.load_model(weights_path)\n",
    "\n",
    "    # Preprocessing\n",
    "    if model_name =='resnet':\n",
    "        preprocessing = res_prep\n",
    "        data = 255*data\n",
    "        data = data - 122\n",
    "        labels = np.reshape(labels,(-1))\n",
    "        #data = tf.cast(data, dtype=tf.float32)\n",
    "        #labels = tf.cast(data, dtype=tf.float32)\n",
    "    elif model_name == 'vgg16':\n",
    "        preprocessing = vgg_prep\n",
    "        data = 255*data\n",
    "        data = data - 122\n",
    "        labels = np.reshape(labels,(-1))\n",
    "        #data = tf.cast(data, dtype=tf.float32)\n",
    "        #labels = tf.cast(data, dtype=tf.float32)\n",
    "    else:\n",
    "        preprocessing = None \n",
    "        data = 255*data\n",
    "        data = data - 122\n",
    "        data = np.expand_dims(data, axis = 3)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_type,\n",
    "                  metrics = metric_list)\n",
    "\n",
    "    model.trainable = False\n",
    "    \n",
    "    \n",
    "    print(\"Evaluate on training data\")\n",
    "    results_train = model.evaluate(tf.convert_to_tensor(data), tf.convert_to_tensor(labels), batch_size=1)\n",
    "    print(\"Loss :{}, Accuracy: {}%.\".format(results_train[0], results_train[1]))\n",
    "    \n",
    "    print(\"Evaluate on testing data\")\n",
    "    results_test = model.evaluate(tf.convert_to_tensor(data_test), tf.convert_to_tensor(labels_test), batch_size=1)\n",
    "    print(\"Loss :{}, Accuracy: {}%.\".format(results_test[0], results_test[1]))\n",
    "    \n",
    "    evaluation_dictionary = {'model': model_name, 'model id': filename, \n",
    "                             'train data': cgf['DATASET_TRAIN']['arguments']['images'], \n",
    "                             'train accuracy': results_train[1], 'train loss': results_train[0],\n",
    "                             'test data': cgf1['DATA']['arguments']['images'],\n",
    "                             'test accuracy': results_test[1], 'test loss': results_test[0]}\n",
    "    \n",
    "    evaluation_of_model = pd.Series(data = evaluation_dictionary, index=['model', 'model id', 'train data',\n",
    "                                                                        'train accuracy', 'train loss',\n",
    "                                                                        'test data', 'test accuracy',\n",
    "                                                                        'test loss'])\n",
    "    evaluation = evaluation.append(evaluation_of_model, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominant-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_pickle('evaluation_of_trained_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-mortality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
